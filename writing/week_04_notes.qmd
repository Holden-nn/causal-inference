---
title: "Linear Models and Causal Inference"
format: gfm
knitr:
  opts_chunk:
    fig.path: "../Figures/"
---

## Chapter 4

> "Linear regression is the geocentric model of applied statistics."

### Normal Distributions

Like the geocentric model, linear regression is stupid in the sense that it might not accurately describe the underlying system but its useful since it can provide helpful descriptive approximations and accurate predictions. Why?

1. Ontologically: Nature is full of normal distributions.
2. Epistemologically: It just requires a mean and a variance.

All of the common statistical procedures are just linear regression.

![](../Figures/meme-regression.png)

### A Common Language

![](../Figures/modeling-language.png)

Now as we move to linear regression, the same principles of the modeling language apply.

```
y_i ∼ Normal(mu, sigma)
mu ∼ Normal(0, 10)
sigma ∼ Uniform(0, 10)
```

A few things to note:

- This model has two parameters. The modeling components and this language generalize no matter the number of parameters.
- Since we have two parameters, the likelihood is the evaluation of the combination of all possible values of the parameters, thus the posterior will be two-dimensional.
- We typically have separate priors for each parameter (or group of parameters), which says the priors are independent of each other.
- We will see grid approximation start to break down as a way to draw samples from the posterior.

### A Model of Height

Let's walk through the example and practice our nascent *workflow*. Note that in the lecture this is a model of **weight**, but the workflow and the thought around the model still applies.

```{r}
# Load packages.
library(rethinking)
library(tidyverse)

# Load the data.
data(Howell1)

# Inspect the data.
as_tibble(Howell1)

# Data prep.
data <- as_tibble(Howell1) |> 
  filter(age >= 18)
```

```
height_i ∼ Normal(mu, sigma)
mu ∼ Normal(178, 20)
sigma ∼ Uniform(0, 50)
```

Are these priors any good? It's time for our first **prior predictive distribution**.

> "By simulating from this distribution, you can see what your choices imply about observable [data]. This helps you diagnose bad choices. Lots of conventional choices are indeed bad ones, and we’ll be able to see this by conducting prior predictive simulations."

```{r}
#| label: week-04-height-prior-pd-01
# Simulate data.
prior_pd <- tibble(
  # 1. Simulate values of mu from its prior.
  mu = rnorm(1000, 178, 20),
  # 2. Simulate values of sigma from its prior.
  sigma = runif(1000, 0, 50),
  # 3. Simulate data conditioned on the prior values.
  height = rnorm(1000, mu, sigma)
)

# Plot the prior predictive distribution.
prior_pd |> 
  ggplot(aes(x = height)) +
  geom_histogram()
```

This prior predictive distribution is the expected distribution of our data, given how we've specified our likelihood and priors. Does this look reasonable? No one has a negative height, for a start. At this point we can iterate on how we've specified our likelihood and priors, produce another prior predictive distribution and evaluate again, etc.

> "Prior predictive simulation is very useful for assigning sensible priors, because it can be quite hard to anticipate how priors influence the observable variables."

Assuming our prior predictive distribution had produced a reasonable expected distribution of the data, let's go straight to computing the posterior distribution using quadratic approximation. Note how the syntax mirrors the model definitions we've already specified.

```{r}
# Fit the model with quadratic approximation.
m4_1 <- quap(
  alist(
    height ~ dnorm(mu, sigma), # height_i ∼ Normal(mu, sigma)
    mu ~ dnorm(178, 20),       # mu ∼ Normal(178, 20)
    sigma ~ dunif(0, 50)       # sigma ∼ Uniform(0, 50)
  ),
  data = data
)

# Quick summary of the posterior.
precis(m4_1)
```

Remember the posterior is two-dimensional (i.e., a multidimensional normal), but here we have estimates for each parameter separately. These are **marginal** posteriors, the slice of the posterior that corresponds to the given parameter.

For a single normal distribution, a mean and variance are sufficient to describe the whole distribution. However, to characterize a multidimensional normal distribution, we need a vector of means and a matrix of variances and covariances.

```{r}
vcov(m4_1)
```

This is a **variance-covariance matrix**. The diagonal are the variances, the off-diagonals are the covariances (and its symmetric across the diagonal). Covariances are weird to interpret, so we typically factor this matrix into the variances and a correlation matrix.

```{r}
diag(vcov(m4_1))
cov2cor(vcov(m4_1))
```

All of this is to say that to sample from the posterior we'll need to draw vectors of values from a multi-dimensional Normal posterior, which will take the variance-covariance matrix into account. Let's do that with `extract.samples()` and summarize our posterior draws.

```{r}
#| label: week-04-height-marginal-posteriors-01
# Sample from the posterior.
posterior <- extract.samples(m4_1, n = 1000)

# Some possible summaries.
precis(posterior)

as_tibble(posterior) |> 
  pivot_longer(
    mu:sigma, 
    names_to = "parameters", 
    values_to = "draws"
  ) |> 
  ggplot(aes(x = draws)) +
  geom_histogram() +
  facet_wrap(~ parameters, scales = "free")
```

### Linear Prediction

This kind of intercept-only model isn't especially interesting. We want to add covariates/independent variables/predictors -- other data that might help explain the observed heights.

> "The strategy is to make the parameter for the mean of a Gaussian distribution, mu, into a linear function of the predictor variable and other, new parameters that we invent. This strategy is often simply called the linear model."

```
height_i ∼ Normal(mu_i, sigma)
mu_i = beta0 + beta1 * (weight_i - avg_weight)
beta0 ∼ Normal(178, 20)
beta1 ~ Normal(0, 10)
sigma ∼ Uniform(0, 50)
```

A few changes from the previous model to note:

- The `i` subscript now applies to `mu` as well.
- `mu` is no longer a parameter. `mu_i` is equal to (not distributed) the linear combination of our parameters and predictors.
- We are centering each `weight_i` such that `beta0` is the effect of the average weight on height (note how if we have the average weight that `beta1 = 0`).
- The prior for the effect of weight on height, `beta1`, is centered around zero as a way to demonstrate we want to learn the sign and size of the effect from the data and to make it easier to set priors.

With more parameters and priors, it becomes even more important to produce a prior predictive distribution to see what this joint model specification implies about reasonable values of the data. **The form of the prior predictive distribution needs to be adapted to what is most informative for the given problem as well as what priors we are evaluating.** For regression, it makes sense to check be composed of regression lines when we are evaluating the priors on `beta0` and `beta1`.

To generate height data, we'll need to draw values of `beta0` and `beta1` along with a range of weights to anchor the two ends, so we'll just use the range of the observed weights. No, this isn't cheating, it's just giving us a reasonable range of weights (the predictor). We're not touching the observed heights (the outcome).

```{r}
#| label: week-04-height-prior-pd-02
# 1. Specify the number of lines to simulate.
N <- 100
prior_pd <- tibble(
  # 2. Specify the line number.
  n = 1:N,
  # 3. Simulate values of beta0 and beta1 from their priors.
  beta0 = rnorm(N, 178, 20), # beta0 ∼ Normal(178, 20)
  beta1 = rnorm(N, 0, 10)    # beta1 ~ Normal(0, 10)
) |>
  # 4. Create two rows for each line, one for each end of the range.
  expand(nesting(n, beta0, beta1), weight = range(data$weight)) |> 
  mutate(
    # 5. Simulate average height using the linear model:
    # mu_i = beta0 + beta1 * (weight_i - avg_weight)
    height = beta0 + beta1 * (weight - mean(data$weight))
  )

# Plot the prior predictive distribution regression lines.
prior_pd |> 
  ggplot(aes(x = weight, y = height, group = n)) +
  geom_line(alpha = 0.10)
```

Note again that this particular prior predictive check has nothing to do with `sigma`. Here we are evaluating the marginal impact of the prior on `beta0` and `beta1` in particular. If we know anything about the relationship between weight and height, we can see that this distribution of the possible relationships between height and weight, as embodied in `beta1`, are unreasonable. We need to revise the prior. Specifically, we have every reason to believe that `beta1` should be strictly positive.

Here's a third attempt at the observational model and priors.

```
height_i ∼ Normal(mu_i, sigma)
mu_i = beta0 + beta1 * (weight_i - avg_weight)
beta0 ∼ Normal(178, 20)
beta1 ~ Log-Normal(0, 1)
sigma ∼ Uniform(0, 50)
```

The change from the previous model is the strictly positive prior on `beta1`. Once again, let's look again at the prior predictive distribution like we saw before.

```{r}
#| label: week-04-height-prior-pd-03
# 1. Specify the number of lines to simulate.
N <- 100
prior_pd <- tibble(
  # 2. Specify the line number.
  n = 1:N,
  # 3. Simulate values of beta0 and beta1 from their priors.
  beta0 = rnorm(N, 178, 20), # beta0 ∼ Normal(178, 20)
  beta1 = rlnorm(N, 0, 1)    # beta1 ~ Log-Normal(0, 1)
) |>
  # 4. Create two rows for each line, one for each end of the range.
  expand(nesting(n, beta0, beta1), weight = range(data$weight)) |> 
  mutate(
    # 5. Simulate average height using the linear model:
    # mu_i = beta0 + beta1 * (weight_i - avg_weight)
    height = beta0 + beta1 * (weight - mean(data$weight)),
  )

# Plot the prior predictive distribution regression lines.
prior_pd |> 
  ggplot(aes(x = weight, y = height, group = n)) +
  geom_line(alpha = 0.10)
```

Much better. The effect of weight on height might be close to zero, but it isn't negative. Let's go ahead and fit the model.

```{r}
# Define the average weight avg_weight.
avg_weight <- mean(data$weight)

# Fit the model with quadratic approximation.
m4_2 <- quap(
  alist(
    height ~ dnorm(mu, sigma),                   # height_i ∼ Normal(mu_i, sigma)
    mu <- beta0 + beta1 * (weight - avg_weight), # mu_i = beta0 + beta1 * (x_i - avg_weight)
    beta0 ~ dnorm(178, 20),                      # beta0 ∼ Normal(178, 20)
    beta1 ~ dlnorm(0, 1),                        # beta1 ~ Log-Normal(0, 1)
    sigma ~ dunif(0, 50)                         # sigma ∼ Uniform(0, 50)
  ),
  data = data
)
```

Let's sample from the posterior.

```{r}
# Sample from the posterior.
posterior <- extract.samples(m4_2, n = 10000)
```

Likelihood, prior, fitting the model and sampling from the posterior -- we have it all! Now we just need to make sense of the posterior. We can first look again at the *marginal* posteriors.

```{r}
#| label: week-04-height-marginal-posteriors-02
# Some possible summaries.
precis(posterior)

as_tibble(posterior) |> 
  gather(key = "parameters", value = "draws") |> 
  ggplot(aes(x = draws)) +
  geom_histogram() +
  facet_wrap(~ parameters, scales = "free")
```

From `beta1`, we can say that a person who is 1 kg heavier is expected to be, on average, 0.90 cm taller. We won't get such an easy interpretation looking at marginal summaries of the other parameters.

Instead of just looking at marginal summaries, we can produce a **posterior predictive distribution**. Instead of manually constructed a distribution of `mu` as we did for the prior predictive distribution, we can use `link()` to generate a posterior predictive distribution of `mu` for us that we could then plot and summarize however we'd like. While this again is about `mu` alone -- nothing about `sigma` -- to construct a *complete* posterior predictive distribution, we need to use the entire posterior, without marginalizing out some parameter or set of parameters.

Fortunately, there is a function for that as well. `sim()` is like `link()`, but we can use it for simulating heights and not just the average height `mu`. We'll make use of both to compare the posterior predictive distribution to the actual data.

```{r}
#| label: week-04-height-posterior-pd-01
# 1. Specify the new data we'd like to use to predict new heights.
posterior_pd <- tibble(new_weights = seq(from = 25, to = 70)) %>% 
  bind_cols(
    # 2. Simulate mu means using the posterior.
    tibble(
      mu_mean = link(m4_2, data = list(weight = .$new_weights)) |>
        apply(2, mean)
    ),
    # 3. Simulate mu intervals using the posterior.
    as_tibble(
      link(m4_2, data = list(weight = .$new_weights)) |> 
      apply(2, PI, prob = 0.99) |> 
      t()
    ),
    # 3. Simulate predicted heights using the entire posterior.
    as_tibble(
      sim(m4_2, data = list(weight = .$new_weights)) |>
      apply(2, PI, prob = 0.89) |>
      t()
    )
  )

# Plot the data along with the posterior predictive distribution.
# Note that the tidyverse equivalent is obtuse and not worth your time (see {ggdist} instead).
plot(data = data, height ~ weight)
lines(posterior_pd$new_weights, posterior_pd$mu_mean)
shade(t(cbind(posterior_pd$`1%`, posterior_pd$`100%`)), posterior_pd$new_weights)
shade(t(cbind(posterior_pd$`5%`, posterior_pd$`94%`)), posterior_pd$new_weights)
```

There are a lot of ways we can visualize the posterior predictive distribution, but ultimately we're just comparing our predicted heights to the heights in the data and determining, yet again, if the model is acting in a reasonable fashion. A general rules is that **our posterior predictive distribution should be of the same form as the corresponding prior predictive distribution**. (The prior and posterior predictive distributions are the first and last panels, respectively, of the Bayesian triptych.)

Note that when we simulate from the posterior to make predictions, we need to be sure to *propagate* our uncertainty about parameters into our uncertainty about predictions. In other words, we need to use the entire posterior distribution. **Using the entire posterior distribution is what makes the analysis Bayesian.**

> "Everything that depends upon parameters has a posterior distribution."

### Categories

There are a number of ways to code (i.e., encode) discrete (i.e., categorical) explanatory variables, with different coding strategies suited for specific use cases. Two of the most common ways to code discrete explanatory variables: dummy coding and index coding. Another common approach is called effects or sum-to-one coding. For a great walkthrough of that approach and its benefits in the context of choice modeling, see Elea Feit's [post](https://eleafeit.com/posts/2021-05-23-parameterization-of-multinomial-logit-models-in-stan/).

#### Dummy Coding

Also known as indicator coding, dummy coding is the most common way to deal with discrete variables, where a single discrete variable with `K` levels is encoded as `K - 1` binary columns, each indicating the presence or absence of the given level. It is an approach to identifying the estimates of discrete explanatory levels that has a specific contrast hard-wired.

If we include all levels of a single discrete variable, they sum up *across columns* to a constant---to an *intercept*. If we did that with more than one discrete variable, we would have more than one intercept and they would no longer be identified. With dummy coding, it is typical to include an intercept (i.e., a constant, often a column of `1`'s) and drop the first level (i.e., the reference level) of each of the discrete variables.

```{r}
# Fit the model with quadratic approximation.
m4_3 <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- beta0 + beta1 * male + beta2 * (weight - avg_weight), 
    beta0 ~ dnorm(178, 20), # What does the intercept mean now exactly?
    beta1 ~ dnorm(0, 10),   # What is the effect of male on height?
    beta2 ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = data
)

# Sample from the posterior.
posterior <- extract.samples(m4_3, n = 10000)

# Quick summary.
precis(posterior)
```

The drawback to dummy coding in a Bayesian setting with *real data* is that we need to specify separate priors over the contrasts rather than the parameters themselves. This complication for setting priors is a good reason to consider effects or index coding. Similarly, the intercept becomes difficulty to interpret.

If you need to dummy-code quickly, `fastDummies::dummy_cols()` is a helpful function.

```{r}
# Define S=1 female and S=2 male as an integer.
data$sex <- data$male + 1

# Dummy code sex to produce a female column.
fastDummies::dummy_cols(data$sex, remove_first_dummy = TRUE)
```

#### Index Coding

Also known as one-hot encoding, index coding similarly turns each level of a discrete variable into its own binary column. However, with index coding we *don't* include an intercept and *don't* include any reference levels. Additionally, we don't have to provide separate binary columns and can simply provide a single vector with the levels of the discrete variable indexed and can set a prior over the entire set of indexed parameters directly.

By not including reference levels, the intercept is implied by the fact that the *implied* columns sum to a constant, as discussed previously. But when we have more than one discrete variable we also have more than one implied intercept. This would create an identification problem in a frequentist setting, but in a Bayesian analysis we simply rely on the prior to enable identification of each of the parameters. As a bonus, the contrasts are always identified even if their constituent parameter estimates are not.

```{r}
# Fit the model with quadratic approximation.
m4_4 <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- alpha[sex] + beta[sex] * (weight - avg_weight),
    alpha[sex] ~ dnorm(178, 20),
    beta[sex] ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = data
)

# Sample from the posterior.
posterior <- extract.samples(m4_4, n = 10000)

# Quick summary.
precis(posterior, depth = 2)
```

Index coding like this is only possible in a Bayesian framework where the use of priors obviates the necessity of strict identification strategies. However, it's not without costs. We will need to specify informative priors, which we should be doing anyway, and the model itself *may* take longer to converge.

If you need to produce index-coded columns instead of a single vector, `fastDummies::dummy_cols()` remains helpful.

```{r}
# Index code sex as separate columns.
fastDummies::dummy_cols(data$sex, remove_first_dummy = FALSE)
```

### Matrix Multiplication

As you start adding many explanatory variables, it may be easier to use matrix multiplication instead of listing each variable and parameter separately in the linear model.

```{r}
# Pay attention to the dimensions of the matrix.
data_mat <- data %>% 
  select(sex, weight) |> 
  as.matrix()

str(data_mat)

# And the dimensions of the betas (as an illustration).
beta_vec <- c(2, 3) |> as.matrix()

str(beta_vec)

# The order of matrix multiplication matters, with dimensions that line up.
Xbeta <- data_mat %*% beta_vec

str(Xbeta)
```

### Curves From Lines

Linear models are additive, but not necessarily *lines*. Allowing for curvature might be helpful. The form of the model (especially the likelihood) depends on the data. However, as we add complexity to the model, issues begin to emerge with overfitting and interpretation (glimpse of the black-box nature of predictive models).

> "Better would be to have a more mechanistic model of the data, one that builds the non-linear relationship up from a principled beginning."

**Polynomial regression** is adding squared (and higher) transformations of predictors to the model.

**Splines** are smooth functions built out of smaller functions. They are a possible improvement over polynomial regression in that they are used to avoid wild swings and can modify local regions of the curve.

- Used for de-trending: Finding an "average" curve so we can consider micro-deviations.
- Often more uncertainty at the parameter level than the prediction level.

## Updated Bayesian Workflow

1. Data Story: Theoretical estimand and causal model.
  + Begin with a conceptual story: Where do these data come from? What does the theory say?
  + What is the causal model? Translate the data story into a DAG.
2. Modeling: Translate the data story into a statistical model.
  + Translate into probability statements (i.e., a model) by identifying variables, both *data* and *parameters*, and how they are distributed.
  + The resulting model is **generative**.
3. Estimation: Design a statistical way to produce the estimate.
  + This is your code as well as the sampling procedure.
  + So far it's grid approximation or quadratic approximation. Eventually, MCMC.
4. Test.
  + Simulate data using the generative model and ensure that you can recover the parameter values you used in simulation.
  + Perform an appropriate **prior predictive check** to evaluate the model. If the resulting distribution of simulated data isn't reasonable, iterate on your model.
5. Analyze real data.
  + Use quadratic approximation to fit the model. Draw samples from the posterior.
  + Perform an appropriate **posterior predictive checks** to evaluate the model. If the resulting distribution of simulated predictions isn't reasonable, iterate on your model.

